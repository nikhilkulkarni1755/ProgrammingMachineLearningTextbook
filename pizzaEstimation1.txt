nikhilkulkarni@Nikhils-MBP SupervisedPizza % python3 
Python 3.8.3 (v3.8.3:6f8c8320e9, May 13 2020, 16:29:34) 
[Clang 6.0 (clang-600.0.57)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> X, Y = np.loadtxt("pizza.txt", skiprows=1, unpack=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'np' is not defined
>>> import numpy as np
>>> X, Y = np.loadtxt("pizza.txt", skiprows=1, unpack=True)
>>> X[0:5]
array([13.,  2., 14., 23., 13.])
>>> Y[0:5]
array([33., 16., 32., 51., 27.])
>>> def predict(X, w):
... return X * w
  File "<stdin>", line 2
    return X * w
    ^
IndentationError: expected an indented block
>>> def predict(X, w):
...     return X * w
... 
>>> def loss(X, Y, w):
...     return np.average((predict(X, w) - Y) ** 2)
... 
>>> def train(X, Y, iterations, lr):
...     w = 0
...     for i in range(iterations):
...             current_loss = loss(X, Y, w)
...             print("Iteration %4d => Loss: %.6f" % (i, current_loss))
...             if loss(X, Y, w + lr) < current_loss:
...                     w += lr
...             elif loss(X, Y, w - lr) < current_loss:
...                     w -= lr
...             else:
...                     return w
...     raise Exception("Couldn't converge within %d iterations" % iterations)
... 
>>> w = train(X, Y, iterations=10000, lr=.01)
Iteration    0 => Loss: 812.866667
Iteration    1 => Loss: 804.820547
Iteration    2 => Loss: 796.818187
Iteration    3 => Loss: 788.859587
Iteration    4 => Loss: 780.944747
Iteration    5 => Loss: 773.073667
Iteration    6 => Loss: 765.246347
Iteration    7 => Loss: 757.462787
Iteration    8 => Loss: 749.722987
Iteration    9 => Loss: 742.026947
Iteration   10 => Loss: 734.374667
Iteration   11 => Loss: 726.766147
Iteration   12 => Loss: 719.201387
Iteration   13 => Loss: 711.680387
Iteration   14 => Loss: 704.203147
Iteration   15 => Loss: 696.769667
Iteration   16 => Loss: 689.379947
Iteration   17 => Loss: 682.033987
Iteration   18 => Loss: 674.731787
Iteration   19 => Loss: 667.473347
Iteration   20 => Loss: 660.258667
Iteration   21 => Loss: 653.087747
Iteration   22 => Loss: 645.960587
Iteration   23 => Loss: 638.877187
Iteration   24 => Loss: 631.837547
Iteration   25 => Loss: 624.841667
Iteration   26 => Loss: 617.889547
Iteration   27 => Loss: 610.981187
Iteration   28 => Loss: 604.116587
Iteration   29 => Loss: 597.295747
Iteration   30 => Loss: 590.518667
Iteration   31 => Loss: 583.785347
Iteration   32 => Loss: 577.095787
Iteration   33 => Loss: 570.449987
Iteration   34 => Loss: 563.847947
Iteration   35 => Loss: 557.289667
Iteration   36 => Loss: 550.775147
Iteration   37 => Loss: 544.304387
Iteration   38 => Loss: 537.877387
Iteration   39 => Loss: 531.494147
Iteration   40 => Loss: 525.154667
Iteration   41 => Loss: 518.858947
Iteration   42 => Loss: 512.606987
Iteration   43 => Loss: 506.398787
Iteration   44 => Loss: 500.234347
Iteration   45 => Loss: 494.113667
Iteration   46 => Loss: 488.036747
Iteration   47 => Loss: 482.003587
Iteration   48 => Loss: 476.014187
Iteration   49 => Loss: 470.068547
Iteration   50 => Loss: 464.166667
Iteration   51 => Loss: 458.308547
Iteration   52 => Loss: 452.494187
Iteration   53 => Loss: 446.723587
Iteration   54 => Loss: 440.996747
Iteration   55 => Loss: 435.313667
Iteration   56 => Loss: 429.674347
Iteration   57 => Loss: 424.078787
Iteration   58 => Loss: 418.526987
Iteration   59 => Loss: 413.018947
Iteration   60 => Loss: 407.554667
Iteration   61 => Loss: 402.134147
Iteration   62 => Loss: 396.757387
Iteration   63 => Loss: 391.424387
Iteration   64 => Loss: 386.135147
Iteration   65 => Loss: 380.889667
Iteration   66 => Loss: 375.687947
Iteration   67 => Loss: 370.529987
Iteration   68 => Loss: 365.415787
Iteration   69 => Loss: 360.345347
Iteration   70 => Loss: 355.318667
Iteration   71 => Loss: 350.335747
Iteration   72 => Loss: 345.396587
Iteration   73 => Loss: 340.501187
Iteration   74 => Loss: 335.649547
Iteration   75 => Loss: 330.841667
Iteration   76 => Loss: 326.077547
Iteration   77 => Loss: 321.357187
Iteration   78 => Loss: 316.680587
Iteration   79 => Loss: 312.047747
Iteration   80 => Loss: 307.458667
Iteration   81 => Loss: 302.913347
Iteration   82 => Loss: 298.411787
Iteration   83 => Loss: 293.953987
Iteration   84 => Loss: 289.539947
Iteration   85 => Loss: 285.169667
Iteration   86 => Loss: 280.843147
Iteration   87 => Loss: 276.560387
Iteration   88 => Loss: 272.321387
Iteration   89 => Loss: 268.126147
Iteration   90 => Loss: 263.974667
Iteration   91 => Loss: 259.866947
Iteration   92 => Loss: 255.802987
Iteration   93 => Loss: 251.782787
Iteration   94 => Loss: 247.806347
Iteration   95 => Loss: 243.873667
Iteration   96 => Loss: 239.984747
Iteration   97 => Loss: 236.139587
Iteration   98 => Loss: 232.338187
Iteration   99 => Loss: 228.580547
Iteration  100 => Loss: 224.866667
Iteration  101 => Loss: 221.196547
Iteration  102 => Loss: 217.570187
Iteration  103 => Loss: 213.987587
Iteration  104 => Loss: 210.448747
Iteration  105 => Loss: 206.953667
Iteration  106 => Loss: 203.502347
Iteration  107 => Loss: 200.094787
Iteration  108 => Loss: 196.730987
Iteration  109 => Loss: 193.410947
Iteration  110 => Loss: 190.134667
Iteration  111 => Loss: 186.902147
Iteration  112 => Loss: 183.713387
Iteration  113 => Loss: 180.568387
Iteration  114 => Loss: 177.467147
Iteration  115 => Loss: 174.409667
Iteration  116 => Loss: 171.395947
Iteration  117 => Loss: 168.425987
Iteration  118 => Loss: 165.499787
Iteration  119 => Loss: 162.617347
Iteration  120 => Loss: 159.778667
Iteration  121 => Loss: 156.983747
Iteration  122 => Loss: 154.232587
Iteration  123 => Loss: 151.525187
Iteration  124 => Loss: 148.861547
Iteration  125 => Loss: 146.241667
Iteration  126 => Loss: 143.665547
Iteration  127 => Loss: 141.133187
Iteration  128 => Loss: 138.644587
Iteration  129 => Loss: 136.199747
Iteration  130 => Loss: 133.798667
Iteration  131 => Loss: 131.441347
Iteration  132 => Loss: 129.127787
Iteration  133 => Loss: 126.857987
Iteration  134 => Loss: 124.631947
Iteration  135 => Loss: 122.449667
Iteration  136 => Loss: 120.311147
Iteration  137 => Loss: 118.216387
Iteration  138 => Loss: 116.165387
Iteration  139 => Loss: 114.158147
Iteration  140 => Loss: 112.194667
Iteration  141 => Loss: 110.274947
Iteration  142 => Loss: 108.398987
Iteration  143 => Loss: 106.566787
Iteration  144 => Loss: 104.778347
Iteration  145 => Loss: 103.033667
Iteration  146 => Loss: 101.332747
Iteration  147 => Loss: 99.675587
Iteration  148 => Loss: 98.062187
Iteration  149 => Loss: 96.492547
Iteration  150 => Loss: 94.966667
Iteration  151 => Loss: 93.484547
Iteration  152 => Loss: 92.046187
Iteration  153 => Loss: 90.651587
Iteration  154 => Loss: 89.300747
Iteration  155 => Loss: 87.993667
Iteration  156 => Loss: 86.730347
Iteration  157 => Loss: 85.510787
Iteration  158 => Loss: 84.334987
Iteration  159 => Loss: 83.202947
Iteration  160 => Loss: 82.114667
Iteration  161 => Loss: 81.070147
Iteration  162 => Loss: 80.069387
Iteration  163 => Loss: 79.112387
Iteration  164 => Loss: 78.199147
Iteration  165 => Loss: 77.329667
Iteration  166 => Loss: 76.503947
Iteration  167 => Loss: 75.721987
Iteration  168 => Loss: 74.983787
Iteration  169 => Loss: 74.289347
Iteration  170 => Loss: 73.638667
Iteration  171 => Loss: 73.031747
Iteration  172 => Loss: 72.468587
Iteration  173 => Loss: 71.949187
Iteration  174 => Loss: 71.473547
Iteration  175 => Loss: 71.041667
Iteration  176 => Loss: 70.653547
Iteration  177 => Loss: 70.309187
Iteration  178 => Loss: 70.008587
Iteration  179 => Loss: 69.751747
Iteration  180 => Loss: 69.538667
Iteration  181 => Loss: 69.369347
Iteration  182 => Loss: 69.243787
Iteration  183 => Loss: 69.161987
Iteration  184 => Loss: 69.123947
>>> print("|nw=%.3f" % w)
|nw=1.840
>>> 
>>> print("Prediction: x = %d => y = %.2f" % (20, predict(20, w)))
Prediction: x = 20 => y = 36.80
>>> w = train(X, Y, iterations=10000, lr=.01)
Iteration    0 => Loss: 812.866667
Iteration    1 => Loss: 804.820547
Iteration    2 => Loss: 796.818187
Iteration    3 => Loss: 788.859587
Iteration    4 => Loss: 780.944747
Iteration    5 => Loss: 773.073667
Iteration    6 => Loss: 765.246347
Iteration    7 => Loss: 757.462787
Iteration    8 => Loss: 749.722987
Iteration    9 => Loss: 742.026947
Iteration   10 => Loss: 734.374667
Iteration   11 => Loss: 726.766147
Iteration   12 => Loss: 719.201387
Iteration   13 => Loss: 711.680387
Iteration   14 => Loss: 704.203147
Iteration   15 => Loss: 696.769667
Iteration   16 => Loss: 689.379947
Iteration   17 => Loss: 682.033987
Iteration   18 => Loss: 674.731787
Iteration   19 => Loss: 667.473347
Iteration   20 => Loss: 660.258667
Iteration   21 => Loss: 653.087747
Iteration   22 => Loss: 645.960587
Iteration   23 => Loss: 638.877187
Iteration   24 => Loss: 631.837547
Iteration   25 => Loss: 624.841667
Iteration   26 => Loss: 617.889547
Iteration   27 => Loss: 610.981187
Iteration   28 => Loss: 604.116587
Iteration   29 => Loss: 597.295747
Iteration   30 => Loss: 590.518667
Iteration   31 => Loss: 583.785347
Iteration   32 => Loss: 577.095787
Iteration   33 => Loss: 570.449987
Iteration   34 => Loss: 563.847947
Iteration   35 => Loss: 557.289667
Iteration   36 => Loss: 550.775147
Iteration   37 => Loss: 544.304387
Iteration   38 => Loss: 537.877387
Iteration   39 => Loss: 531.494147
Iteration   40 => Loss: 525.154667
Iteration   41 => Loss: 518.858947
Iteration   42 => Loss: 512.606987
Iteration   43 => Loss: 506.398787
Iteration   44 => Loss: 500.234347
Iteration   45 => Loss: 494.113667
Iteration   46 => Loss: 488.036747
Iteration   47 => Loss: 482.003587
Iteration   48 => Loss: 476.014187
Iteration   49 => Loss: 470.068547
Iteration   50 => Loss: 464.166667
Iteration   51 => Loss: 458.308547
Iteration   52 => Loss: 452.494187
Iteration   53 => Loss: 446.723587
Iteration   54 => Loss: 440.996747
Iteration   55 => Loss: 435.313667
Iteration   56 => Loss: 429.674347
Iteration   57 => Loss: 424.078787
Iteration   58 => Loss: 418.526987
Iteration   59 => Loss: 413.018947
Iteration   60 => Loss: 407.554667
Iteration   61 => Loss: 402.134147
Iteration   62 => Loss: 396.757387
Iteration   63 => Loss: 391.424387
Iteration   64 => Loss: 386.135147
Iteration   65 => Loss: 380.889667
Iteration   66 => Loss: 375.687947
Iteration   67 => Loss: 370.529987
Iteration   68 => Loss: 365.415787
Iteration   69 => Loss: 360.345347
Iteration   70 => Loss: 355.318667
Iteration   71 => Loss: 350.335747
Iteration   72 => Loss: 345.396587
Iteration   73 => Loss: 340.501187
Iteration   74 => Loss: 335.649547
Iteration   75 => Loss: 330.841667
Iteration   76 => Loss: 326.077547
Iteration   77 => Loss: 321.357187
Iteration   78 => Loss: 316.680587
Iteration   79 => Loss: 312.047747
Iteration   80 => Loss: 307.458667
Iteration   81 => Loss: 302.913347
Iteration   82 => Loss: 298.411787
Iteration   83 => Loss: 293.953987
Iteration   84 => Loss: 289.539947
Iteration   85 => Loss: 285.169667
Iteration   86 => Loss: 280.843147
Iteration   87 => Loss: 276.560387
Iteration   88 => Loss: 272.321387
Iteration   89 => Loss: 268.126147
Iteration   90 => Loss: 263.974667
Iteration   91 => Loss: 259.866947
Iteration   92 => Loss: 255.802987
Iteration   93 => Loss: 251.782787
Iteration   94 => Loss: 247.806347
Iteration   95 => Loss: 243.873667
Iteration   96 => Loss: 239.984747
Iteration   97 => Loss: 236.139587
Iteration   98 => Loss: 232.338187
Iteration   99 => Loss: 228.580547
Iteration  100 => Loss: 224.866667
Iteration  101 => Loss: 221.196547
Iteration  102 => Loss: 217.570187
Iteration  103 => Loss: 213.987587
Iteration  104 => Loss: 210.448747
Iteration  105 => Loss: 206.953667
Iteration  106 => Loss: 203.502347
Iteration  107 => Loss: 200.094787
Iteration  108 => Loss: 196.730987
Iteration  109 => Loss: 193.410947
Iteration  110 => Loss: 190.134667
Iteration  111 => Loss: 186.902147
Iteration  112 => Loss: 183.713387
Iteration  113 => Loss: 180.568387
Iteration  114 => Loss: 177.467147
Iteration  115 => Loss: 174.409667
Iteration  116 => Loss: 171.395947
Iteration  117 => Loss: 168.425987
Iteration  118 => Loss: 165.499787
Iteration  119 => Loss: 162.617347
Iteration  120 => Loss: 159.778667
Iteration  121 => Loss: 156.983747
Iteration  122 => Loss: 154.232587
Iteration  123 => Loss: 151.525187
Iteration  124 => Loss: 148.861547
Iteration  125 => Loss: 146.241667
Iteration  126 => Loss: 143.665547
Iteration  127 => Loss: 141.133187
Iteration  128 => Loss: 138.644587
Iteration  129 => Loss: 136.199747
Iteration  130 => Loss: 133.798667
Iteration  131 => Loss: 131.441347
Iteration  132 => Loss: 129.127787
Iteration  133 => Loss: 126.857987
Iteration  134 => Loss: 124.631947
Iteration  135 => Loss: 122.449667
Iteration  136 => Loss: 120.311147
Iteration  137 => Loss: 118.216387
Iteration  138 => Loss: 116.165387
Iteration  139 => Loss: 114.158147
Iteration  140 => Loss: 112.194667
Iteration  141 => Loss: 110.274947
Iteration  142 => Loss: 108.398987
Iteration  143 => Loss: 106.566787
Iteration  144 => Loss: 104.778347
Iteration  145 => Loss: 103.033667
Iteration  146 => Loss: 101.332747
Iteration  147 => Loss: 99.675587
Iteration  148 => Loss: 98.062187
Iteration  149 => Loss: 96.492547
Iteration  150 => Loss: 94.966667
Iteration  151 => Loss: 93.484547
Iteration  152 => Loss: 92.046187
Iteration  153 => Loss: 90.651587
Iteration  154 => Loss: 89.300747
Iteration  155 => Loss: 87.993667
Iteration  156 => Loss: 86.730347
Iteration  157 => Loss: 85.510787
Iteration  158 => Loss: 84.334987
Iteration  159 => Loss: 83.202947
Iteration  160 => Loss: 82.114667
Iteration  161 => Loss: 81.070147
Iteration  162 => Loss: 80.069387
Iteration  163 => Loss: 79.112387
Iteration  164 => Loss: 78.199147
Iteration  165 => Loss: 77.329667
Iteration  166 => Loss: 76.503947
Iteration  167 => Loss: 75.721987
Iteration  168 => Loss: 74.983787
Iteration  169 => Loss: 74.289347
Iteration  170 => Loss: 73.638667
Iteration  171 => Loss: 73.031747
Iteration  172 => Loss: 72.468587
Iteration  173 => Loss: 71.949187
Iteration  174 => Loss: 71.473547
Iteration  175 => Loss: 71.041667
Iteration  176 => Loss: 70.653547
Iteration  177 => Loss: 70.309187
Iteration  178 => Loss: 70.008587
Iteration  179 => Loss: 69.751747
Iteration  180 => Loss: 69.538667
Iteration  181 => Loss: 69.369347
Iteration  182 => Loss: 69.243787
Iteration  183 => Loss: 69.161987
Iteration  184 => Loss: 69.123947
>>> print(predict(40, w))
73.60000000000005
>>> 40 * w
73.60000000000005
>>> 
